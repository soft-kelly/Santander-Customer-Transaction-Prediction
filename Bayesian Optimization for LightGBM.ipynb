{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1ef572ea1f522bcdc6a775bf629ee740d87ac"
   },
   "source": [
    "# Bayesian Optimization for LightGBM\n",
    "\n",
    "In this notebook I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be rank average on 5 fold cross validation predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7409352edc21584713b1225028f087c6989e94be"
   },
   "source": [
    "## Notebook  Content\n",
    "0. [Installing Bayesian global optimization library](#0) <br>    \n",
    "1. [Loading the data](#1)\n",
    "2. [Black box function to be optimized (LightGBM)](#2)\n",
    "3. [Training LightGBM model](#3)\n",
    "4. [Rank averaging](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## 0. Installing Bayesian Optimization library\n",
    "\n",
    "Let's install the latest release from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "81c2a7386319cae39c1cf83d39394d530f549128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /anaconda3/lib/python3.6/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (1.15.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (0.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import rankdata\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f5624fd428106888ec023312d3479d324ef0eac9"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6f7cd5bdd625e69c75e10f586a826da80c814cdf"
   },
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "predictors = train_df.columns.values.tolist()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "496907f42901e10bb883858252b2783e30ff2e43"
   },
   "source": [
    "In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7ec87a3460c6358b9a134afea5bac561f7a84226"
   },
   "outputs": [],
   "source": [
    "bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbcebd0aacaeb637a1e119971303c9fcd60f9ea5"
   },
   "source": [
    "These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f18184730f7261c3ddc2253ee025f9910ffedb6"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2. Black box function to be optimized (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a5be20e9809494709c5da85861775c8720816ba"
   },
   "source": [
    "As data is loaded, let's create the black box function for LightGBM to find parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "196288ebb7caf614e230a0449b40354266efbc45"
   },
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    num_leaves,  # int\n",
    "    min_data_in_leaf,  # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf,    # int  \n",
    "    feature_fraction,\n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. So we make them integer\n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "\n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'bagging_fraction': 1.0,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,   \n",
    "\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_tr_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_val_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    num_round = 5000\n",
    "    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    \n",
    "    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n",
    "    \n",
    "    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40b3959e5f0672b6e030408cd197da137e1b66ee"
   },
   "source": [
    "The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n",
    "\n",
    "The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fb52d3d130c2477a6ab71b2d6797c787dce9f21"
   },
   "source": [
    "Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "044ed09d293f7a712af6fe6c00e935df19ca1cf4"
   },
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (5, 20), \n",
    "    'min_data_in_leaf': (5, 20),  \n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n",
    "    'feature_fraction': (0.05, 0.5),\n",
    "    'lambda_l1': (0, 5.0), \n",
    "    'lambda_l2': (0, 5.0), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(3,15),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a2a217d1bcc8b27e67b9cc7f4b7bbb237b8ee72"
   },
   "source": [
    "Let's put all of them in BayesianOptimization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c0fc26566d5b5bf64a9f507cd06bf0ab85b584e3"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "af6565a9d43d2b5a153a3f2009afca7618a699c1"
   },
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f597ee9781e5621c98fe89d69a30601902c716"
   },
   "source": [
    "Now, let's the the key space (parameters) we are going to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "281f53a7d9bca23329e965986939443bc63a927c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_fraction', 'lambda_l1', 'lambda_l2', 'learning_rate', 'max_depth', 'min_data_in_leaf', 'min_gain_to_split', 'min_sum_hessian_in_leaf', 'num_leaves']\n"
     ]
    }
   ],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2fbbcbe963f659df1884511af1a0009f28ef6d5"
   },
   "source": [
    "I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n",
    "- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n",
    "- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e06903e320665c4f77e9b029681cf6f651dfc9b4"
   },
   "source": [
    "Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "6c12e5f092c2d0f690fd720e40aec69268c9e70f"
   },
   "outputs": [],
   "source": [
    "init_points = 5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f5f6d3f1a7752d818330b92980344956bf934e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's auc: 0.878546\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 1.188   \u001b[0m | \u001b[0m 4.121   \u001b[0m | \u001b[0m 0.2901  \u001b[0m | \u001b[0m 14.67   \u001b[0m | \u001b[0m 11.8    \u001b[0m | \u001b[0m 0.609   \u001b[0m | \u001b[0m 0.007758\u001b[0m | \u001b[0m 14.62   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.84073\n",
      "[500]\tvalid_0's auc: 0.866356\n",
      "[750]\tvalid_0's auc: 0.877017\n",
      "[1000]\tvalid_0's auc: 0.882954\n",
      "[1250]\tvalid_0's auc: 0.886643\n",
      "[1500]\tvalid_0's auc: 0.888876\n",
      "[1750]\tvalid_0's auc: 0.890244\n",
      "[2000]\tvalid_0's auc: 0.891098\n",
      "[2250]\tvalid_0's auc: 0.891491\n",
      "Early stopping, best iteration is:\n",
      "[2387]\tvalid_0's auc: 0.891578\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8916  \u001b[0m | \u001b[95m 0.3749  \u001b[0m | \u001b[95m 0.1752  \u001b[0m | \u001b[95m 1.492   \u001b[0m | \u001b[95m 0.02697 \u001b[0m | \u001b[95m 13.28   \u001b[0m | \u001b[95m 10.59   \u001b[0m | \u001b[95m 0.6798  \u001b[0m | \u001b[95m 0.00257 \u001b[0m | \u001b[95m 10.21   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.879455\n",
      "[500]\tvalid_0's auc: 0.889134\n",
      "[750]\tvalid_0's auc: 0.892162\n",
      "Early stopping, best iteration is:\n",
      "[802]\tvalid_0's auc: 0.89247\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8925  \u001b[0m | \u001b[95m 0.05424 \u001b[0m | \u001b[95m 1.792   \u001b[0m | \u001b[95m 4.745   \u001b[0m | \u001b[95m 0.07319 \u001b[0m | \u001b[95m 6.833   \u001b[0m | \u001b[95m 18.77   \u001b[0m | \u001b[95m 0.0319  \u001b[0m | \u001b[95m 0.000660\u001b[0m | \u001b[95m 14.45   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.879482\n",
      "[500]\tvalid_0's auc: 0.884038\n",
      "Early stopping, best iteration is:\n",
      "[555]\tvalid_0's auc: 0.884316\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8843  \u001b[0m | \u001b[0m 0.4432  \u001b[0m | \u001b[0m 0.04358 \u001b[0m | \u001b[0m 3.733   \u001b[0m | \u001b[0m 0.2457  \u001b[0m | \u001b[0m 3.909   \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 0.5093  \u001b[0m | \u001b[0m 0.004804\u001b[0m | \u001b[0m 19.33   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.881955\n",
      "[500]\tvalid_0's auc: 0.890201\n",
      "[750]\tvalid_0's auc: 0.89183\n",
      "Early stopping, best iteration is:\n",
      "[794]\tvalid_0's auc: 0.891892\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8919  \u001b[0m | \u001b[0m 0.05001 \u001b[0m | \u001b[0m 1.235   \u001b[0m | \u001b[0m 3.561   \u001b[0m | \u001b[0m 0.1041  \u001b[0m | \u001b[0m 6.324   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 0.9186  \u001b[0m | \u001b[0m 0.002452\u001b[0m | \u001b[0m 11.87   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.882933\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's auc: 0.887774\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8878  \u001b[0m | \u001b[0m 0.2888  \u001b[0m | \u001b[0m 0.06764 \u001b[0m | \u001b[0m 0.06368 \u001b[0m | \u001b[0m 0.2366  \u001b[0m | \u001b[0m 8.822   \u001b[0m | \u001b[0m 19.91   \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.009826\u001b[0m | \u001b[0m 5.699   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.885102\n",
      "Early stopping, best iteration is:\n",
      "[405]\tvalid_0's auc: 0.88983\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8898  \u001b[0m | \u001b[0m 0.05    \u001b[0m | \u001b[0m 4.998   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2826  \u001b[0m | \u001b[0m 3.166   \u001b[0m | \u001b[0m 5.462   \u001b[0m | \u001b[0m 6.064e-1\u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.870052\n",
      "[500]\tvalid_0's auc: 0.885015\n",
      "[750]\tvalid_0's auc: 0.88999\n",
      "[1000]\tvalid_0's auc: 0.89117\n",
      "Early stopping, best iteration is:\n",
      "[1077]\tvalid_0's auc: 0.891403\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8914  \u001b[0m | \u001b[0m 0.2457  \u001b[0m | \u001b[0m 0.4681  \u001b[0m | \u001b[0m 4.634   \u001b[0m | \u001b[0m 0.1089  \u001b[0m | \u001b[0m 5.749   \u001b[0m | \u001b[0m 5.199   \u001b[0m | \u001b[0m 0.03918 \u001b[0m | \u001b[0m 0.007056\u001b[0m | \u001b[0m 5.492   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.861183\n",
      "[500]\tvalid_0's auc: 0.879321\n",
      "[750]\tvalid_0's auc: 0.886825\n",
      "[1000]\tvalid_0's auc: 0.890166\n",
      "[1250]\tvalid_0's auc: 0.891956\n",
      "[1500]\tvalid_0's auc: 0.893104\n",
      "Early stopping, best iteration is:\n",
      "[1516]\tvalid_0's auc: 0.893113\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8931  \u001b[0m | \u001b[95m 0.07253 \u001b[0m | \u001b[95m 4.779   \u001b[0m | \u001b[95m 3.22    \u001b[0m | \u001b[95m 0.07664 \u001b[0m | \u001b[95m 15.0    \u001b[0m | \u001b[95m 6.072   \u001b[0m | \u001b[95m 0.4635  \u001b[0m | \u001b[95m 0.007511\u001b[0m | \u001b[95m 5.002   \u001b[0m |\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.878413\n",
      "[500]\tvalid_0's auc: 0.886043\n",
      "Early stopping, best iteration is:\n",
      "[517]\tvalid_0's auc: 0.88625\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8862  \u001b[0m | \u001b[0m 0.3822  \u001b[0m | \u001b[0m 4.973   \u001b[0m | \u001b[0m 0.07274 \u001b[0m | \u001b[0m 0.2226  \u001b[0m | \u001b[0m 3.35    \u001b[0m | \u001b[0m 19.87   \u001b[0m | \u001b[0m 0.9903  \u001b[0m | \u001b[0m 0.002911\u001b[0m | \u001b[0m 19.94   \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdea40277b15b11b5b92238e519cf5fc2576f64f"
   },
   "source": [
    "As the optimization is done, let's see what is the maximum value we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d0d5a3fe41a7d9f1625cdfb5eceb113f6cabcf93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931128566288707"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad0a200a70e00955a924ca5f3fc622b492eaf3e3"
   },
   "source": [
    "The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "ced952bfd9397a68e3a2d394a4f7e075b40a9aff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.0725311213806508,\n",
       " 'lambda_l1': 4.778909939998174,\n",
       " 'lambda_l2': 3.220122059503974,\n",
       " 'learning_rate': 0.07663544326133201,\n",
       " 'max_depth': 14.99504485809263,\n",
       " 'min_data_in_leaf': 6.071757005556453,\n",
       " 'min_gain_to_split': 0.4634735801572327,\n",
       " 'min_sum_hessian_in_leaf': 0.007510919138894667,\n",
       " 'num_leaves': 5.001626877554514}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f6bc1d0859b643bdbbeba1e0e3c8b336860c023"
   },
   "source": [
    "Now we can use these parameters to our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06096bd955639e891320f5b31408fad1785f5949"
   },
   "source": [
    "Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a0275beb85200e0c8cce95b477dd756e8d1ef2ef"
   },
   "outputs": [],
   "source": [
    "# parameters from version 2 of\n",
    "#https://www.kaggle.com/fayzur/customer-transaction-prediction?scriptVersionId=10522231\n",
    "\n",
    "LGB_BO.probe(\n",
    "    params={'feature_fraction': 0.1403, \n",
    "            'lambda_l1': 4.218, \n",
    "            'lambda_l2': 1.734, \n",
    "            'learning_rate': 0.07, \n",
    "            'max_depth': 14, \n",
    "            'min_data_in_leaf': 17, \n",
    "            'min_gain_to_split': 0.1501, \n",
    "            'min_sum_hessian_in_leaf': 0.000446, \n",
    "            'num_leaves': 6},\n",
    "    lazy=True, # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2893bc7de2bf69fd02ef26e90fc354f71b83e10"
   },
   "source": [
    "OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8e3feb820520bb03000225416e338610e2d90b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86243\n",
      "[500]\tvalid_0's auc: 0.880738\n",
      "[750]\tvalid_0's auc: 0.887841\n",
      "[1000]\tvalid_0's auc: 0.890905\n",
      "[1250]\tvalid_0's auc: 0.892214\n",
      "[1500]\tvalid_0's auc: 0.892655\n",
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's auc: 0.892672\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8927  \u001b[0m | \u001b[0m 0.1403  \u001b[0m | \u001b[0m 4.218   \u001b[0m | \u001b[0m 1.734   \u001b[0m | \u001b[0m 0.07    \u001b[0m | \u001b[0m 14.0    \u001b[0m | \u001b[0m 17.0    \u001b[0m | \u001b[0m 0.1501  \u001b[0m | \u001b[0m 0.000446\u001b[0m | \u001b[0m 6.0     \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bd70047d5313694681d73ed049339d6b00261c0"
   },
   "source": [
    "Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "416bfb9650849e75106ee63b40dfd7519aa2ee29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8785456504868869, 'params': {'feature_fraction': 0.3999660847582191, 'lambda_l1': 1.1877061001745615, 'lambda_l2': 4.1213926633068425, 'learning_rate': 0.2900672674324699, 'max_depth': 14.67121336685872, 'min_data_in_leaf': 11.801738711259683, 'min_gain_to_split': 0.6090424627612779, 'min_sum_hessian_in_leaf': 0.007757509880902418, 'num_leaves': 14.624200171386038}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8915776403640969, 'params': {'feature_fraction': 0.3749082032826262, 'lambda_l1': 0.17518262050718658, 'lambda_l2': 1.492247354445897, 'learning_rate': 0.026968622645801674, 'max_depth': 13.284731311046386, 'min_data_in_leaf': 10.592810418122113, 'min_gain_to_split': 0.679847951578097, 'min_sum_hessian_in_leaf': 0.002570236693773035, 'num_leaves': 10.21371822728738}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8924701144136038, 'params': {'feature_fraction': 0.05423574653643624, 'lambda_l1': 1.7916689135248487, 'lambda_l2': 4.745470908391052, 'learning_rate': 0.07319071264818977, 'max_depth': 6.8326963965643746, 'min_data_in_leaf': 18.76658579000881, 'min_gain_to_split': 0.03190366643989473, 'min_sum_hessian_in_leaf': 0.0006601945250547198, 'num_leaves': 14.447434986617345}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.884315595674114, 'params': {'feature_fraction': 0.4432160494757924, 'lambda_l1': 0.04357866151892431, 'lambda_l2': 3.7328861849696877, 'learning_rate': 0.24572393959076078, 'max_depth': 3.9086093540672007, 'min_data_in_leaf': 14.846830018954368, 'min_gain_to_split': 0.5092622000835182, 'min_sum_hessian_in_leaf': 0.004804035081048867, 'num_leaves': 19.333612173965996}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8918922376241951, 'params': {'feature_fraction': 0.0500054151062683, 'lambda_l1': 1.2348935049595817, 'lambda_l2': 3.5611633895579176, 'learning_rate': 0.1041287944381468, 'max_depth': 6.323956276605713, 'min_data_in_leaf': 15.431681788302118, 'min_gain_to_split': 0.9185517481459488, 'min_sum_hessian_in_leaf': 0.0024523122649579236, 'num_leaves': 11.871287259151455}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8877741089318032, 'params': {'feature_fraction': 0.28880235499863005, 'lambda_l1': 0.0676352739721825, 'lambda_l2': 0.06367783660314708, 'learning_rate': 0.23658833958998987, 'max_depth': 8.82154329429286, 'min_data_in_leaf': 19.911222977094404, 'min_gain_to_split': 0.7962518122529608, 'min_sum_hessian_in_leaf': 0.00982563254531773, 'num_leaves': 5.699237677594455}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8898304036671378, 'params': {'feature_fraction': 0.05, 'lambda_l1': 4.998389870187783, 'lambda_l2': 0.0, 'learning_rate': 0.2826314994674229, 'max_depth': 3.165595420297774, 'min_data_in_leaf': 5.461850359793226, 'min_gain_to_split': 6.063870788414655e-10, 'min_sum_hessian_in_leaf': 1.0001168447710937e-05, 'num_leaves': 5.0}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8914029939138292, 'params': {'feature_fraction': 0.24571681728644235, 'lambda_l1': 0.46805485709572603, 'lambda_l2': 4.634252859543099, 'learning_rate': 0.10893673688261334, 'max_depth': 5.74869199423548, 'min_data_in_leaf': 5.199199561225826, 'min_gain_to_split': 0.03917651107600029, 'min_sum_hessian_in_leaf': 0.007056042775055096, 'num_leaves': 5.49237053333988}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.8931128566288707, 'params': {'feature_fraction': 0.0725311213806508, 'lambda_l1': 4.778909939998174, 'lambda_l2': 3.220122059503974, 'learning_rate': 0.07663544326133201, 'max_depth': 14.99504485809263, 'min_data_in_leaf': 6.071757005556453, 'min_gain_to_split': 0.4634735801572327, 'min_sum_hessian_in_leaf': 0.007510919138894667, 'num_leaves': 5.001626877554514}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.8862497365758225, 'params': {'feature_fraction': 0.3822100269594015, 'lambda_l1': 4.9725933326472855, 'lambda_l2': 0.07274421757568761, 'learning_rate': 0.22264841127984392, 'max_depth': 3.350373855747472, 'min_data_in_leaf': 19.869533531527104, 'min_gain_to_split': 0.9903177231349555, 'min_sum_hessian_in_leaf': 0.002911082950989143, 'num_leaves': 19.93785226420804}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8926723153666577, 'params': {'feature_fraction': 0.1403, 'lambda_l1': 4.218, 'lambda_l2': 1.734, 'learning_rate': 0.07, 'max_depth': 14.0, 'min_data_in_leaf': 17.0, 'min_gain_to_split': 0.1501, 'min_sum_hessian_in_leaf': 0.000446, 'num_leaves': 6.0}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(LGB_BO.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32a1bd2e328c121915b77177e72f281e6b64ca4c"
   },
   "source": [
    "We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5e881cbd9dc198f4cc91bb65823542ec473c6b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931128566288707"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "1a132698b07c7652d62c329f5d6508211540834f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.0725311213806508,\n",
       " 'lambda_l1': 4.778909939998174,\n",
       " 'lambda_l2': 3.220122059503974,\n",
       " 'learning_rate': 0.07663544326133201,\n",
       " 'max_depth': 14.99504485809263,\n",
       " 'min_data_in_leaf': 6.071757005556453,\n",
       " 'min_gain_to_split': 0.4634735801572327,\n",
       " 'min_sum_hessian_in_leaf': 0.007510919138894667,\n",
       " 'num_leaves': 5.001626877554514}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e17fc50406f38145fffae3c4aa585f5520433cab"
   },
   "source": [
    "Let's build a model together use therse parameters ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b91db8b5c98da4f014f1863ba7de18e241f517c6"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3. Training LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "56d006d366fc5c2686249887b5d1f302d4a708f5"
   },
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n",
    "        'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n",
    "        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5a87047368fe7504c895aab4e9deb1b43746d7d"
   },
   "source": [
    "As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ab8ef627687c7602b81a9ac83f90ff3a2094d0c"
   },
   "source": [
    "Number of Kfolds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9f306e20ae748715da17a2f15702ea1aa4d81497"
   },
   "outputs": [],
   "source": [
    "nfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f2f01a6006dcb34ffb53ca33a055e592ee9e75e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.863083\n",
      "[500]\tvalid_0's auc: 0.882892\n",
      "[750]\tvalid_0's auc: 0.890602\n",
      "[1000]\tvalid_0's auc: 0.894356\n",
      "[1250]\tvalid_0's auc: 0.896589\n",
      "[1500]\tvalid_0's auc: 0.89754\n",
      "Early stopping, best iteration is:\n",
      "[1658]\tvalid_0's auc: 0.89802\n",
      "\n",
      "fold 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.858944\n",
      "[500]\tvalid_0's auc: 0.878526\n",
      "[750]\tvalid_0's auc: 0.887018\n",
      "[1000]\tvalid_0's auc: 0.891189\n",
      "[1250]\tvalid_0's auc: 0.893594\n",
      "[1500]\tvalid_0's auc: 0.894757\n",
      "[1750]\tvalid_0's auc: 0.895592\n",
      "[2000]\tvalid_0's auc: 0.896141\n",
      "Early stopping, best iteration is:\n",
      "[2009]\tvalid_0's auc: 0.89618\n",
      "\n",
      "fold 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.865096\n",
      "[500]\tvalid_0's auc: 0.884309\n",
      "[750]\tvalid_0's auc: 0.891393\n",
      "[1000]\tvalid_0's auc: 0.895089\n",
      "[1250]\tvalid_0's auc: 0.897185\n",
      "[1500]\tvalid_0's auc: 0.898671\n",
      "[1750]\tvalid_0's auc: 0.899236\n",
      "Early stopping, best iteration is:\n",
      "[1836]\tvalid_0's auc: 0.899408\n",
      "\n",
      "fold 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.857489\n",
      "[500]\tvalid_0's auc: 0.877766\n",
      "[750]\tvalid_0's auc: 0.885996\n",
      "[1000]\tvalid_0's auc: 0.890003\n",
      "[1250]\tvalid_0's auc: 0.891775\n",
      "[1500]\tvalid_0's auc: 0.892999\n",
      "Early stopping, best iteration is:\n",
      "[1685]\tvalid_0's auc: 0.893563\n",
      "\n",
      "fold 5\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.858071\n",
      "[500]\tvalid_0's auc: 0.879376\n",
      "[750]\tvalid_0's auc: 0.88667\n",
      "[1000]\tvalid_0's auc: 0.890783\n",
      "[1250]\tvalid_0's auc: 0.893165\n",
      "[1500]\tvalid_0's auc: 0.894253\n",
      "[1750]\tvalid_0's auc: 0.894975\n",
      "Early stopping, best iteration is:\n",
      "[1911]\tvalid_0's auc: 0.895205\n",
      "\n",
      "\n",
      "CV AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros((len(test_df),nfold))\n",
    "\n",
    "i = 1\n",
    "for train_index, valid_index in skf.split(train_df, train_df.target.values):\n",
    "    print(\"\\nfold {}\".format(i))\n",
    "    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n",
    "                           label=train_df.iloc[train_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n",
    "                           label=train_df.iloc[valid_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    \n",
    "    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n",
    "    \n",
    "    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\\nCV AUC: {:<0.3f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6c8fde3d71858028688bd9a62bf1a4430b6bbb1"
   },
   "source": [
    "So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a889d2c1f63dbe989b3f0b0373ce9e1c80e3ee10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47431024, 0.39227261, 0.44468045, 0.38681385, 0.42184034],\n",
       "       [0.68469791, 0.6810922 , 0.70206808, 0.58120489, 0.66681126],\n",
       "       [0.732413  , 0.72693412, 0.59743011, 0.6587553 , 0.74861721],\n",
       "       ...,\n",
       "       [0.0649998 , 0.03296316, 0.04015894, 0.04720749, 0.02271844],\n",
       "       [0.40141865, 0.43569536, 0.4639441 , 0.46392935, 0.37243934],\n",
       "       [0.44078843, 0.39342836, 0.42481913, 0.49840102, 0.42225307]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01339e95305a72802fd23a2c740cb7cc988d1c27"
   },
   "source": [
    "If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "116f2c0c3c43095d456a88d50425de0a3e7fdd11"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## 4. Rank averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6ec40259b4636edd2c336583c5eb5c62feaaaa31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank averaging on 5 fold predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank averaging on\", nfold, \"fold predictions\")\n",
    "rank_predictions = np.zeros((predictions.shape[0],1))\n",
    "for i in range(nfold):\n",
    "    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))/rank_predictions.shape[0]) \n",
    "\n",
    "rank_predictions /= nfold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
